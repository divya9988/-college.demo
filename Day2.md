Training Day 2 Report
Date: 24 June 2025

Introduction to Large Language Models (LLMs)
On the second day of training, we learned about Large Language Models (LLMs). LLMs are advanced AI models trained on huge datasets to understand and generate human-like language. They can perform a wide range of tasks such as writing, summarizing, translating, coding, and answering questions.

Examples: GPT (by OpenAI), LLaMA (by Meta), Gemini (by Google).

Key Terms
Token
A token is the smallest unit of text processed by the model. It can be a word or part of a word.
Analogy: Like pieces of a puzzle that together form a complete picture (sentence).

Parameter
Parameters are internal values the model learns during training to capture language patterns.
Analogy: Like knobs on a radio that you adjust to get a clear sound; parameters help fine-tune the model’s understanding.

Prompt
A prompt is the instruction or input given to the model to generate a response.
Analogy: Like a question asked to a teacher — a clear question gets a clear answer.

Fine-Tuning
Fine-tuning means taking a pre-trained model and training it further on specific data to specialize it.
Analogy: Like teaching a student who knows general science to focus deeply on biology.

Inference
Inference is the process of using a trained model to generate output from new prompts.
Analogy: Like asking a trained chef to cook a new dish from a recipe — they apply their skills to create the final result.

Transformers
Transformers are the core architecture behind most LLMs. Introduced in 2017, they revolutionized natural language processing. Transformers use a mechanism called attention, which helps the model focus on important words in a sentence and understand relationships between them.
Example: In the sentence "The girl who won the prize is my sister," the model understands that "who won the prize" refers to "the girl" using attention.

Training LLMs: Behind the Scenes
LLMs are trained using massive datasets that include books, websites, articles, and more. The training involves:
Feeding large amounts of text to the model.
Adjusting billions of parameters to minimize errors.
Using high-powered GPUs and large-scale computing resources.
The entire process can take weeks or months and requires significant computing power and energy.

Applications of LLMs
Content creation: Writing articles, blogs, scripts.
Customer support: Chatbots and virtual assistants.
Code generation: Helping programmers write and debug code.
Education: Assisting students with explanations and summaries.
Translation: Converting text between languages.
Creative work: Generating stories, poems, and even art prompts.

Limitations of LLMs
May produce incorrect or biased information.
Cannot truly understand context like a human.
Require large computational resources.
Depend heavily on the quality of training data.
Sometimes fail at tasks that need common sense reasoning.

Ethical Concerns
Bias: Models can reflect biases present in training data.
Misinformation: Can unintentionally generate false or misleading information.
Privacy: Risk of reproducing sensitive data from training sets.
Job displacement: Automation may affect certain jobs.

Future of LLMs
More accurate and context-aware models.
Better safety and bias reduction techniques.
Improved energy efficiency and faster inference times.
Wider adoption in education, healthcare, and creative industries.
Development of specialized models fine-tuned for specific professional domains.

Key Takeaways
Understood the concept of LLMs and their working.
Learned important key terms with helpful analogies.
Explored the role of transformers and attention mechanisms.
Got insights into training processes behind LLMs.
Discussed practical applications, limitations, and ethical issues.
Understood possible future advancements and potential impact on society.

By: Divya
URN: 2302514
CRN: 2315057
Section: CSEA2
